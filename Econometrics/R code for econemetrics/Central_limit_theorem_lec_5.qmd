---
title: "Central limit theorem"
format: pdf
editor: visual
---

## Central limit theorem

```{r}
rm(list = ls())
library(tidyverse)
library(extraDistr)
```

In this simulation, we repeatedly compute the average of 100 dice rolls 10000 times, and plot the results in a histogram.

Run the following R code:

```{r}
Dice <- function(n) rdunif(n, 1, 6) %>% mean()
R1 <- numeric(10000)
for(i in 1:10000) R1[i] <- Dice(100)
hist(R1, freq = FALSE)
```

The histogram looks almost symmetric and unimodal with its peak at 𝔼(𝑋) = 3.5.

Next, we perform a coin flipping experiment: head = 1, tail = 0.

Similarly as above, compute the average of 100 coin flipping results 10000 times, and plot them in a histogram.

```{r}
Coin <- function(n) rdunif(n, 0, 1) %>% mean()
R2 <- numeric(10000)
for(i in 1:10000) R2[i] <- Coin(100)
hist(R2, freq = FALSE)
```

Again, the histogram looks almost symmetric and unimodal with its peak at 𝔼(𝑋) = 0.5.

The most important probability distribution in the entire field of statistics is the normal distribution.

The normal distribution is symmetric and unimodal. The shape of the normal distribution is fully characterized by two parameters: the mean 𝜇 and the standard deviation 𝜎. The mean 𝜇 determines the center of the distribution, and the standard deviation 𝜎 determines the width of the curve.

```{r}
par(mfrow = c(1,3)) # split the graphic window into (1,3)
curve(dnorm(x, mean = 0, sd = 1), xlim = c(-4,10), ylim = c(0,0.6))
curve(dnorm(x, mean = 2, sd = 0.7), xlim = c(-4,10), ylim = c(0,0.6))
curve(dnorm(x, mean = 3, sd = 2), xlim = c(-4,10), ylim = c(0,0.6))
```

The normal distribution with mean 𝜇 and standard deviation 𝜎 is denoted as 𝑁(𝜇, 𝜎2), and its PDF is given by:

$$f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$

If X follows the normal distribution N (𝜇, 𝜎2), the probability that X is less than or equal to $\alpha$ is given by the CDF of the normal distribution:

$$F(\alpha) = \int_{-\infty}^{\alpha} f(x)dx = \int_{-\infty}^{\alpha} \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}dx$$

where $F(\alpha)$ is the CDF of the normal distribution. $N(\sigma, \mu^2)$.In particular the normal distribution $N(0,1)$ is called the standard normal distribution.

It seems possible to approximate the histogram of the average dice rolls by an “appropriately chosen” normal distribution

```{r}
hist(R1, freq = FALSE, xlim = c(2,5), ylim = c(0,2.5))
par(new = T)
curve(dnorm(x, 3.5, 0.171), xlim = c(2,5), ylim = c(0,2.5), col = "red")
```

The same is true for the coin flipping experiment

```{r}
hist(R2, freq= FALSE, xlim = c(0.2,0.8), ylim = c(0,10))
par(new = T)
curve(dnorm(x, 0.5, 0.05), xlim = c(0.2,0.8), ylim = c(0,10), col = "red")
```

Dice rolling and coin flipping experiments have different probability distributions, but the distribution of the sample mean can be approximated by a normal distribution in both cases.

This surprising result is due to the central limit theorem.

The central limit theorem states that the distribution of the sample mean of a large number of independent, identically distributed random variables approaches a normal distribution, regardless of the shape of the original distribution.

Suppose we have data $X_1, X_2, ..., X_n$ of sample size $n$ randomly drawn from the same population. Let pop mean of $X_i$ be $\mu$ and pop variance of $X_i$ be $\sigma^2$. Then the sample mean $\bar{X}$ and sample variance $S^2$ are given by:

$$\bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_i$$

Then if $n$ is large, the distribution of $\bar{X}$ is approximately normal with mean $\mu$ and variance $\frac{\sigma^2}{n}$.

## Convergence in distribution

A sequence of random variables $(v_n)^\infty_{n^= 1}$ is said to converge in distribution to $N(\mu, \sigma^2)$ if

$$\lim_{n \to \infty} P(v_n \leq x) = \phi(x;u,\sigma)$$

for any $x$.

When $v_n$ converges in distribution to $N(\mu, \sigma^2)$, we write $v_n \xrightarrow{d} N(\mu, \sigma^2)$.

The following are equivalent:

1.  $v_n \xrightarrow{d} N(\mu, \sigma^2)$

$\mu$ and $\sigma$ are the mean and standard deviation of $v_n$, respectively

A formal statement of CLT is the following: under the aforementioned assumptions,

$$\sqrt{n}(\bar{X} - \mu) \xrightarrow{d} N(0, \sigma^2)$$

where $\bar{X}$ is the sample mean of $X_i$.

What is remarkable about the central limit theorem is that it does not require the underlying distribution to be normal. The central limit theorem is a fundamental result in probability theory and statistics, and it is the reason why the normal distribution is so important in the field of statistics.

The proof is very hard and omitted.

## Convergence in distribution

Dice roll simulation

$E[X] = 3.5$

$Var[X] = \frac{35}{12} = \sqrt{Var[X]} = 1.71$

```{r}
plot_Dice <- function(n){
 R <- numeric(10000)
 for(i in 1:10000) R[i] <- Dice(n)
 hist(sqrt(n)*(R - 3.5)/1.71, freq = FALSE, xlim = c(-3,3), ylim = c(0, 0.5))
 par(new = T)
 curve(dnorm(x), xlim = c(-3,3), ylim = c(0, 0.5), col = "red")
 }
par(mfrow = c(1,3))
plot_Dice(3); plot_Dice(8); plot_Dice(500)
```

Coin toss simulation:

$E[X] = 0.5$

$Var[X] = (0 - 0.5)^2/2 + (1 - 0.5)^2/2 = 0.25$, $\sqrt{Var[X]} = 0.5$

```{r}
plot_Coin <- function(n){
 R <- numeric(10000)
 for(i in 1:10000) R[i] <- Coin(n)
 hist(sqrt(n)*(R - 0.5)/0.5, freq = FALSE, xlim = c(-3,3), ylim = c(0, 0.5))
 par(new = T)
 curve(dnorm(x), xlim = c(-3,3), ylim = c(0, 0.5), col = "red")
}

par(mfrow = c(1,3))

plot_Coin(3); plot_Coin(8); plot_Coin(500)
```

Confidence interval

Let Z be distributed as the standard normal N(0,1). Then it holds that $P(-1.96 \leq Z \leq 1.96) = 0.95$.

That is, the area of grayed part in the following figure is equal to 0.95.

```{r}
curve(dnorm(x), xlim = c(-3,3), ylim = c(0, 0.5))
# density of standard normal distribution
polygon(c(-1.96, seq(-1.96, 1.96, 0.01), 1.96), c(0, dnorm(seq(-1.96, 1.96, 0.01)), 0), col = "gray", title("95 percent"))


```

Similarly, the 99% confidence interval is $(-2.58, 2.58)$.

```{r}
curve(dnorm(x), xlim = c(-3,3), ylim = c(0, 0.5))
# density of standard normal distribution
polygon(c(-2.58, seq(-2.58, 2.58, 0.01), 2.58), c(0, dnorm(seq(-2.58, 2.58, 0.01)), 0), col = "gray", title("99 percent"))
```

```{r}
integrate(dnorm, -1.96, 1.96)
integrate(dnorm, -2.58, 2.58)
```

Let {X1, X2, ..., Xn} be a random sample of n observations. According to the central limit theorem, the distribution of the standardized sample mean:

$$\frac{\sqrt{n}(\bar{X_n}- \mu}{\sigma}$$

can be approximated by the standard normal distribution N(0,1) when n is large.

Thus we have the following:

$$P(-1.96 \leq \frac{\sqrt{n}(\bar{X_n}- \mu}{\sigma} \leq 1.96) = 0.95$$

for succiently large n.

Hence if n is sufficiencly large, we have

$$P(\bar{X_n} - \frac{1.96\sigma}{\sqrt{n}} \leq \mu \leq \bar{X_n} + \frac{1.96\sigma}{\sqrt{n}}) = 0.95$$

This is the 95% confidence interval for the population mean $\mu$.

And this implies that the population mean $\mu$ is in the interval $[\bar{X_n} - \frac{1.96\sigma}{\sqrt{n}}, \bar{X_n} + \frac{1.96\sigma}{\sqrt{n}}]$ with probability 0.95.

Even though we do not know the true value of 𝜇, the interval in which 𝜇 is contained with a certain probability is computable.

This is the power of the central limit theorem.

In the dice roll experiment the 95% CI is $[\bar{X_n}- \frac{3.35}{\sqrt{n}}, \bar{X_n} + \frac{3.35}{\sqrt{n}}]$.

We computate the CI 10000 times and check how many times the true value 3.5 is in the CI for different n.

```{r}
CI <- function(n) Dice(n) + c(-3.35, 3.35)/sqrt(n)

test <- function(n){
  result <- numeric(10000)
  for(i in 1:10000){
    CI <- CI(n)
    result[i] <- ifelse(CI[1] <= 3.5 & 3.5 <= CI[2], 1, 0)
  }
  mean(result)
}

test(5)
test(10)
test(1000)
```

Thus as n gets larger the probability that the true value is in the CI converges to 0.95.

A common misunderstanding:

$\mu$ is a fixed value, and it is not a random variable. The CI is a random variable, and it is the interval in which $\mu$ is contained with a certain probability.

## t-distirbution

In order to compute the CI for the population mean, we need to know the population standard deviation $\sigma$. However, in practice, we do not know the population standard deviation. Instead, we use the sample standard deviation $s$ as an estimate of $\sigma$.

The distribution of the standardized sample mean:

$$\hat{\sigma} = \sqrt{\frac{1}{n-1}} \sum_{i=1}^{n} (X_i - \bar{X_n})^2 $$

and estimate the CI by:

$$\hat{CI}_n = [\bar{X}_n - 1.96 \frac{\hat{\sigma}}{\sqrt{n}}, \bar{X}_n + 1.96 \frac{\hat{\sigma}}{\sqrt{n}}]  $$ When $\sigma$ is replaced by $\hat{\sigma}_n$, the distribution of the standardized sample mean follows the t-distribution with n-1 degrees of freedom.

The t-distribution has a similar shape as the standard normal distribution, and converges to 𝑁(0, 1) as 𝑛 increases.

```{r}
# plot curves together
curve(dnorm(x), xlim = c(-3, 3), ylim = c(0, 0.5), ylab = "Density", main = "Density Functions")
curve(dt(x, df = 1), col = "red", add = TRUE)
curve(dt(x, df = 2), col = "blue", add = TRUE)
curve(dt(x, df = 100), col = "green", add = TRUE)
legend("topright", legend = c("Normal", "t, df=1", "t, df=2", "t, df=100"), 
       col = c("black", "red", "blue", "green"), lty = 1)

```

Thus as n increases, the t-distribution converges to the standard normal distribution.
