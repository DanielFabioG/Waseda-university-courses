---
title: "Assignment 3 quarto"
format: pdf
editor: visual
warning: false
---

# WEEK 10: Homework Assignments

For this assignment, you’ll need to start with a blank le in R, and gure out from there what code you’ll need to solve the following problems. As ever, submit your R file through Moodle to complete the assignment.

Figuring out which of the techniques we have used thus far is the core point of this assignment - it’s a test of how well you have understood regression techniques, not a test of your programming abilities, and you won’t be graded harshly for programming mistakes.

This assignment is worth twice as much as previous homework assignments, and has an extended deadline of January 5th

\newpage

# Assignment 1: Vocational School Graduate Income

The le grad_income.csv contains a large number of observations of the income of vocational school graduates in a certain country. The data includes the gender and age of each person, along with their annual income.

Your task is to analyse this data and nd the most appropriate linear model to explain the differences in income.

Some questions you should consider as you’re looking for the best model:

-   Is the relationship between age and income linear, or might it be described better by a curvilinear model?
-   Is the relationship between gender and income independent of the relationship with age, or is there an interaction between these terms?

## 1) Find the most appropriate and effective model for this data. Show your workings in your R code (i.e what other models you tried, and any tests you did to compare models.)

```{r}
# Load the data
options(scipen = 999)
rm(list = ls())
library(tidyverse)
library(stargazer)
url <- "https://raw.githubusercontent.com/DanielFabioG/data/refs/heads/main/grad_income_2.csv"
grad_income <- read.csv(url)

# Check the data
summary(grad_income)
```

First looking at the linear model

```{r, warning=FALSE}
# Fit a linear model
linear_model <- (lm(income ~ age, data = grad_income))

# Check the model with stargazer
stargazer(linear_model, type = "text")
```

```{r}
# GGplot the model
grad_income %>% 
ggplot(aes(x = age, y = income)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  theme_minimal()
```

Looking at the quadratic model

```{r, warning=FALSE}
# Fit a quadratic model
grad_income$gender <- factor(grad_income$gender)


grad_income$age2 <- grad_income$age^2
quad_model <- lm(income ~ age + age2, data = grad_income)


# Check both models with stargazer
stargazer(linear_model, quad_model, type = "text")
```

```{r}
# GGplot the quadratic model
grad_income %>%
  ggplot(aes(x = age, y = income)) +
  geom_point() +
  geom_smooth(method = "lm", 
              formula = y ~ x + I(x^2), 
              color = "red")+
  theme_minimal()
```

Continuing with looking at cubic model

```{r, warning=FALSE}
# Fit a polynomial model

poly_model <- lm(income ~ poly(age, 3), data = grad_income)

# Check all models with stargazer
stargazer(linear_model, quad_model, poly_model, type = "text")
```

```{r}
# Ggplot the cubic model

grad_income %>%
  ggplot(aes(x = age, y = income)) +
  geom_point() +
  geom_smooth(method = "lm", 
              formula = y ~ poly(x, 3), 
              color = "red")+
  theme_minimal()
```

```{r}
# Code copied from week 09 R assignment to look for the best polynomial model
poly_results <- tibble(
  Level = numeric(),
  AdjR2 = numeric(),
  Significance = numeric(),
  Stars = character()
)
for (polycount in 10:3) {
  fit <- lm(income ~ poly(age, polycount), data = grad_income)
  signif <- summary(fit)$coefficients[polycount + 1, 4]
  poly_results <- add_row(poly_results, 
                          Level = polycount,
                          AdjR2 = summary(fit)$adj.r.squared,
                          Significance = signif,
                          Stars = case_when(
                            signif < 0.01 ~ "***",
                            signif < 0.05 ~ " **",
                            signif < 0.1 ~ "  *",
                            TRUE ~ "   "
                          ))
}
poly_results
```

The linear model is the worst, while the best model seems to be the cubic model with a slightly higher adjusted R-squared value and a lower p-value than the quadratic model.

Lets continue and look at the log and exp model

```{r, warning=FALSE}
# Fit a exp
exp_model <- lm(log(income) ~ age, data = grad_income)

# Fit a log model
log_model <- lm(income ~ log(age), data = grad_income)

# log log model
log_log_model <- lm(log(income) ~ log(age), data = grad_income)

# Check poly model with stargazer
stargazer(exp_model, log_model,log_log_model, type = "text")
```

Log and exp models are worse as I expected.

Lastly looking at reciprocal model

```{r, warning=FALSE}
# Fit a reciprocal model
reciprocal_model <- lm(income ~ I(1/age), data = grad_income)

# Check poly model vs reciprocal with stargazer
stargazer(poly_model,reciprocal_model, type = "text")
```

Cubic model is conslusively the best model.

Now looking if there is a relationship in the model between age and gender

```{r, warning = FALSE}
# Fit a model with interaction
interaction_model <- lm(income ~ age + gender, data = grad_income)

# cubic model with interaction
interaction_cubic_model <- lm(income ~ poly(age, 3) + gender, data = grad_income)

# Check the model with stargazer
stargazer(interaction_model, interaction_cubic_model, type = "text")
```

## 2) Write a few lines justifying your choice of model.

The interaction cubic model is the best model as it has the highest adjusted R-squared value and the lowest p-value, it also includes the interaction between age and gender which is important as the relationship between age and income is different between what type of gender the person is.

## 3) Create a graph which illustrates the relationship between age, gender, and income.

```{r}
# Ggplot the interaction cubic model
grad_income %>%
  ggplot(aes(x = age, y = income, color = gender)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", 
              formula = y ~ poly(x, 3),
              se = FALSE) +
  scale_color_manual(values = c("palevioletred", "cornflowerblue")) +
  # fixing the y scale values to dollars
  scale_y_continuous(labels = scales::dollar_format()) +
  # fixing the x scale for age
  scale_x_continuous(breaks = seq(0, 100, 10)) +
  labs(title = "Relationship between income, age and gender",
       x = "Age",
       y = "Income",
       color = "") +
  theme_minimal()+
  theme(legend.position = "bottom")
```

\newpage

# Assignment 2: Titanic Survivors

The file titanic.csv contains the passenger data from the Titanic, including their survival or nonsurvival of the ship’s sinking. We saw in class that paying more for a ticket increased the chances of survival, as did being female.

Your task is to look at a few additional factors and see how they may have impacted the chances of surviving the Titanic. Bear in mind that many of these columns will need to be turned into factors before they can be effectively used in a regression. Also take careful note of the data in your outcome variable, and make sure you’re using the right regression model.

## 1) The column `pclass` tells us which class the passenger was in - the highest class is rst, with third-class being the cheapest accommodation on the ship. We know `fare` is correlated with survival; we want to know if this is just because the more expensive first-class accommodation was better positioned for survival.

Test whether `pclass` is also correlated with survival, and see if `fare` is still significant even once `pclass` is included.

```{r}
# Load the data
url <- "https://raw.githubusercontent.com/DanielFabioG/data/refs/heads/main/titanic.csv"

titanic <- read_csv(url)

# Check the data
glimpse(titanic)
```

```{r, warning=FALSE}

# survived and pclass are not factors but R understands 
# them as factors since they are 1 and 0 and 1,2,3 respectively

# Fit a model with pclass and fare
pclass_model <- glm(survived ~ pclass + fare, family = binomial(), data = titanic)
fare_model <- glm(survived ~ fare, family = binomial(), data = titanic)

# Check the model with stargazer
stargazer(fare_model,pclass_model, type = "text")
```

Fare is correlated with survival looking at the model with only fare, but when pclass is included fare is no longer significant, but pclass is.

\newpage

## 2) “Women and children first” was the famous policy for boarding lifeboats in shipwrecks at this time. We found that women were much more likely to survive than men, but what about children?

To test this, you will need create a new factor variable that separates out women, men, and children (we probably don’t care about the gender of children, so we can just treat them as a third separate category).

To construct this new column, I suggest that you look for information about the ifelse() function (which we discussed a little in class) and the case_when() function. You’ll need to use a combination of information from the `age` column and the `sex` column.

Once it’s created, test whether being a woman or a child makes a major difference to survival.

```{r}
# Creating new column for men, women and children
titanic <- titanic %>%
  mutate(
    person_type = case_when(
      age < 16                  ~ "child",
      sex == "female"          ~ "woman",
      sex == "male" | is.na(sex) ~ "man"   # Catch any NA for safety
    ),
    # make this a factor for cleaner analyses
    person_type = factor(person_type, levels = c("man", "woman", "child"))
  )
```

```{r, warning=FALSE}
# Fit a model with person_type
person_type_model <- glm(survived ~ person_type, family = binomial(), data = titanic)

# Check the model with stargazer
stargazer(person_type_model, type = "text")

```

```{r}
chisq.test(table(titanic$person_type, titanic$survived))
```

The chi-squared test shows that there is a significant difference between the survival of male, female and children.

```{r}
xtabs(~person_type+survived, data = titanic)
```

The table shows that being a woman or a child makes a major difference to survival than being a man. It also shows that being a woman is more likely to survive than being a child.

```{r}
anova(pclass_model, person_type_model, test = "Chisq")
```

Looking at the models we can see that the person_type model is better than the pclass model, as the large drop in residual deviance shows, which means the person_type model is a better fit for the data. We cannot see the p-value for the anova test, because the person_type model is not nested in the pclass model.

\newpage

## 3) Finally, combine these factors and find the most effective model to explain the odds of surviving the Titanic, based on whatever you think is the most effective combination of ticket price, passenger class, gender, and age. You don’t have to include all these factors if you don’t think they’re all necessary.

```{r, warning=FALSE}
# Fit a model with person_type, pclass and fare
persontypefare <- glm(survived ~ person_type * pclass + pclass + fare, family = binomial()
                         , data = titanic)

# Check the model with stargazer
stargazer(persontypefare, type = "text")

```

```{r}
anova(person_type_model, persontypefare, test = "Chisq")
```

Seems like adding it all together plus the interaction between person_type and pclass is a better fit for the data, as the large drop in residual deviance shows. The p-value is also significant, which means the final model is a better fit for the data than the person_type model.

Lastly I will make some new variables to see if they can improve the model even further.

```{r}
# Creating a new categorical variable for family size or if the person is alone

titanic <- titanic %>%
  mutate(
    family_size = sibsp + parch,
    is_alone = ifelse(family_size == 0, "alone", "not_alone"),
    is_alone = factor(is_alone)
  )

# Creating a new variable for looking if living closer to the deck had an impact on survival
titanic <- titanic %>%
  mutate(
    deck = substr(cabin, 1, 1), # Adding na to the missing values
    deck = ifelse(is.na(deck), "no_cabin", deck),
    deck = factor(deck)
  )

# Lastly I will extract the title from the name column
titanic <- titanic %>%
  mutate(
    title = gsub("(.*, )|(\\..*)", "", name),
    title = factor(title)
  )
```

```{r, warning=FALSE}
# Fit a model with person_type, pclass, fare, family_size, is_alone, deck and title
final_model <- glm(survived ~ person_type * pclass + fare + family_size + is_alone + deck + title,
                   family = binomial()
                   , data = titanic)

# Check the model with stargazer
stargazer(final_model, type = "text")
```

```{r}
# looking at the categorical variables to see if the model is overfitting
table(titanic$deck, titanic$survived)
table(titanic$title, titanic$survived)
table(titanic$is_alone, titanic$survived)
```

```{r}
anova(persontypefare, final_model, test = "Chisq")
```

The model is improved but it got worse by overfitting the data as many of the new categories have too few observations to be significant. The p-value is also not significant, which means the final model is not a better fit for the data than the persontypefare model at the moment, I will try to group them better to see if the model improves correctly.

```{r}
titanic <- titanic %>%
  mutate(deck_grouped = case_when(
    deck %in% c("G","T","F") ~ "rare_deck",
    deck %in% c("A","B","C","D","E") ~ as.character(deck),  # keep them
    deck == "no_cabin" ~ "no_cabin",
    TRUE ~ deck  # just in case
  )) 

titanic <- titanic %>%
  mutate(deck_grouped = factor(deck_grouped))

# Combining the rare titles

titanic <- titanic %>%
  mutate(title_grouped = case_when(
    title %in% c("Dr", "Rev", "Major", "Col", "Capt", "Jonkheer", "Don", "Sir") ~ "rare_title",
    title %in% c("Mme", "Ms", "Lady", "Mlle", "the Countess", "Dona") ~ "Miss",
    TRUE ~ title))

# looking at the categorical variables to see if the model is overfitting
table(titanic$deck_grouped, titanic$survived)
table(titanic$title_grouped, titanic$survived)

```

```{r, warning=FALSE}
# Fit a model with person_type, pclass, fare, family_size, is_alone, deck_grouped and title_grouped
final_model_2 <- glm(survived ~ person_type * pclass + fare + family_size + is_alone + deck_grouped + title_grouped,
                   family = binomial()
                   , data = titanic)

# Check the model with stargazer
stargazer(final_model_2, type = "text")
```

```{r}
anova(persontypefare, final_model_2, test = "Chisq")
```

Even though the p-value is significant and the model is improved, the final model is not a good enough improvement to the persontypemodel. Personally I think the model is too complex and gets improved too little for it to be worth to devide the categories even further. Lastly I will remove some of the variables that are not significant and see how much I can remove to get a better model that is not overfitting the data.

```{r, warning=FALSE}
simple_model <- glm(
  survived ~ person_type * pclass + pclass + family_size + is_alone, 
  family = binomial(link = "logit"), 
  data = titanic
)

stargazer(persontypefare, simple_model, type = "text")
```

```{r}
anova(persontypefare, simple_model, test = "Chisq")
```

## 4) Write a few lines justifying your choice of this model.

I chose the "simple model" because it is the best fit for the data, as it improves the model significantly while removing the term fare which didnt seem to fit the model and adds family size and shows if the passenger were alone, which seems to improve it also.

The p-value is also significant, which means the simple model is a better fit for the data than the persontypefare model. The simple model is also not overfitting the data, as the p-values for the variables are significant and the model is not too complex. The simple model is also relatively easy to interpret and understand, which is important when presenting the results to others. The final_model_2 is too complex and does not improve the model enough to be worth the extra complexity in my opinion.

Link to ChatGpt:

<https://chatgpt.com/share/67698ad5-7ce0-8001-8ca1-e27d9e51654b>
